{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "try:\n",
        "    import comet_ml\n",
        "except ImportError:\n",
        "    comet_ml = None\n",
        "try:\n",
        "    import wandb\n",
        "except ImportError:\n",
        "    wandb = None\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "except ImportError:\n",
        "    plt = None\n",
        "    logging.warning(\"Matplotlib not installed. This is not needed if you run this script as --headless\")\n",
        "\n",
        "import lightning as L\n",
        "from lightning.pytorch import Trainer, seed_everything\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch import loggers as pl_loggers\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from models import imagebind_model\n",
        "from models import lora as LoRA\n",
        "from models.imagebind_model import ModalityType, load_module, save_module\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, force=True)\n",
        "\n",
        "# Logging settings\n",
        "LOG_ON_STEP = True\n",
        "LOG_ON_EPOCH = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "USi_lzakW2bj",
        "outputId": "2d227a4b-0ba0-4b79-97e4-b364b27a75da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lightning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3c6dc4b21261>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Matplotlib not installed. This is not needed if you run this script as --headless\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveTransformations:\n",
        "    def __init__(self, base_transforms, n_views=2):\n",
        "        self.base_transforms = base_transforms\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transforms(x) for _ in range(self.n_views)]\n",
        "\n",
        "\n",
        "class ImageBindTrain(L.LightningModule):\n",
        "    def __init__(self, lr=5e-4, weight_decay=1e-4, max_epochs=500, batch_size=32, num_workers=4, seed=42,\n",
        "                 self_contrast=False, temperature=0.07,  momentum_betas=(0.9, 0.95),\n",
        "                 lora=False, lora_rank=4, lora_checkpoint_dir=\"./.checkpoints/lora\",\n",
        "                 lora_layer_idxs=None, lora_modality_names=None,\n",
        "                 linear_probing=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        assert not (linear_probing and lora), \\\n",
        "            \"Linear probing is a subset of LoRA training procedure for ImageBind. \" \\\n",
        "            \"Cannot set both linear_probing=True and lora=True. \" \\\n",
        "            \"Linear probing stores params in lora_checkpoint_dir\"\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Load full pretrained ImageBind model\n",
        "        self.model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "        if lora:\n",
        "            for modality_preprocessor in self.model.modality_preprocessors.children():\n",
        "                modality_preprocessor.requires_grad_(False)\n",
        "            for modality_trunk in self.model.modality_trunks.children():\n",
        "                modality_trunk.requires_grad_(False)\n",
        "\n",
        "            self.model.modality_trunks.update(LoRA.apply_lora_modality_trunks(self.model.modality_trunks, rank=lora_rank,\n",
        "                                                                              layer_idxs=lora_layer_idxs,\n",
        "                                                                              modality_names=lora_modality_names))\n",
        "            LoRA.load_lora_modality_trunks(self.model.modality_trunks, checkpoint_dir=lora_checkpoint_dir)\n",
        "\n",
        "            # Load postprocessors & heads\n",
        "            load_module(self.model.modality_postprocessors, module_name=\"postprocessors\",\n",
        "                        checkpoint_dir=lora_checkpoint_dir)\n",
        "            load_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=lora_checkpoint_dir)\n",
        "        elif linear_probing:\n",
        "            for modality_preprocessor in self.model.modality_preprocessors.children():\n",
        "                modality_preprocessor.requires_grad_(False)\n",
        "            for modality_trunk in self.model.modality_trunks.children():\n",
        "                modality_trunk.requires_grad_(False)\n",
        "            for modality_postprocessor in self.model.modality_postprocessors.children():\n",
        "                modality_postprocessor.requires_grad_(False)\n",
        "\n",
        "            load_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=lora_checkpoint_dir)\n",
        "            for modality_head in self.model.modality_heads.children():\n",
        "                modality_head.requires_grad_(False)\n",
        "                final_layer = list(modality_head.children())[-1]\n",
        "                final_layer.requires_grad_(True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay,\n",
        "                                betas=self.hparams.momentum_betas)\n",
        "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr / 50\n",
        "        )\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def info_nce_loss(self, batch, mode=\"train\"):\n",
        "        data_a, class_a, data_b, class_b= batch\n",
        "\n",
        "        # class_a is always \"vision\" according to ImageBind\n",
        "        feats_a = [self.model({class_a[0]: data_a_i}) for data_a_i in data_a]\n",
        "        feats_a_tensor = torch.cat([list(dict_.values())[0] for dict_ in feats_a], dim=0)\n",
        "        # class_b could be any modality\n",
        "        feats_b = [self.model({class_b[idx]: data_b_i}) for idx, data_b_i in enumerate(data_b)]\n",
        "        feats_b_tensor = torch.cat([list(dict_.values())[0] for dict_ in feats_b], dim=0)\n",
        "\n",
        "        if self.hparams.self_contrast:\n",
        "            feats_a_b_tensor = torch.cat([feats_a_tensor.chunk(2)[0], feats_b_tensor], dim=0)\n",
        "            feats_tensors = [feats_a_tensor, feats_a_b_tensor]\n",
        "            temperatures = [1, self.hparams.temperature]\n",
        "            contrast = [\"self\", \"cross\"]\n",
        "        else:\n",
        "            feats_a_b_tensor = torch.cat([feats_a_tensor, feats_b_tensor], dim=0)\n",
        "            feats_tensors = [feats_a_b_tensor]\n",
        "            temperatures = [self.hparams.temperature]\n",
        "            contrast = [\"cross\"]\n",
        "\n",
        "        # Accumulate self-contrastive loss for image and its augmentation, and modailty with image\n",
        "        dual_nll = False\n",
        "        for feats_idx, feats_tensor in enumerate(feats_tensors):\n",
        "            # Calculate cosine similarity\n",
        "            cos_sim = F.cosine_similarity(feats_tensor[:, None, :], feats_tensor[None, :, :], dim=-1)\n",
        "            # Mask out cosine similarity to itself\n",
        "            self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
        "            cos_sim.masked_fill_(self_mask, -9e15)\n",
        "            # Find positive example -> batch_size//2 away from the original example\n",
        "            pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
        "            # InfoNCE loss\n",
        "            cos_sim = cos_sim / temperatures[feats_idx]\n",
        "            nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
        "            nll = nll.mean()\n",
        "            if not dual_nll:\n",
        "                dual_nll = nll\n",
        "            else:\n",
        "                dual_nll += nll\n",
        "                dual_nll /= 2\n",
        "            # Logging loss\n",
        "            self.log(mode + \"_loss_\" + contrast[feats_idx], nll, prog_bar=True,\n",
        "                     on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "            # Get ranking position of positive example\n",
        "            comb_sim = torch.cat(\n",
        "                [cos_sim[pos_mask][:, None], cos_sim.masked_fill(pos_mask, -9e15)],  # First position positive example\n",
        "                dim=-1,\n",
        "            )\n",
        "            sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
        "            # Logging ranking metrics\n",
        "            self.log(mode + \"_acc_top1\", (sim_argsort == 0).float().mean(), prog_bar=True,\n",
        "                     on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "            self.log(mode + \"_acc_top5\", (sim_argsort < 5).float().mean(), prog_bar=True,\n",
        "                     on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "            self.log(mode + \"_acc_mean_pos\", 1 + sim_argsort.float().mean(), prog_bar=True,\n",
        "                     on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "\n",
        "        self.log(mode + \"_loss\", dual_nll, prog_bar=True,\n",
        "                 on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "        return dual_nll\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.info_nce_loss(batch, mode=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.info_nce_loss(batch, mode=\"val\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.hparams.lora:\n",
        "            # Save LoRA checkpoint\n",
        "            LoRA.save_lora_modality_trunks(self.model.modality_trunks, checkpoint_dir=self.hparams.lora_checkpoint_dir)\n",
        "            # Save postprocessors & heads\n",
        "            save_module(self.model.modality_postprocessors, module_name=\"postprocessors\",\n",
        "                        checkpoint_dir=self.hparams.lora_checkpoint_dir)\n",
        "            save_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=self.hparams.lora_checkpoint_dir)\n",
        "        elif self.hparams.linear_probing:\n",
        "            # Save postprocessors & heads\n",
        "            save_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=self.hparams.lora_checkpoint_dir)\n"
      ],
      "metadata": {
        "id": "q7jQXeB7ZCbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train the ImageBind model with PyTorch Lightning and LoRA.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=43, help=\"Random seed for reproducibility\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cpu\", help=\"Device to use for training ('cpu' or 'cuda')\")\n",
        "    parser.add_argument(\"--datasets_dir\", type=str, default=\"./.datasets\",\n",
        "                        help=\"Directory containing the datasets\")\n",
        "\n",
        "    # Adding MELD as a dataset argument\n",
        "    parser.add_argument(\"--datasets\", type=str, nargs=\"+\", default=[\"MELD\"], choices=[\"MELD\", \"dreambooth\"],\n",
        "                        help=\"Datasets to use for training and validation\")\n",
        "\n",
        "\n",
        "    parser.add_argument(\"--full_model_checkpoint_dir\", type=str, default=\"./.checkpoints/full\",\n",
        "                        help=\"Directory to save the full model checkpoints\")\n",
        "    parser.add_argument(\"--full_model_checkpointing\", action=\"store_true\", help=\"Save full model checkpoints\")\n",
        "    parser.add_argument(\"--loggers\", type=str, nargs=\"+\", choices=[\"tensorboard\", \"wandb\", \"comet\", \"mlflow\"],\n",
        "                        help=\"Loggers to use for logging\")\n",
        "    parser.add_argument(\"--loggers_dir\", type=str, default=\"./.logs\", help=\"Directory to save the logs\")\n",
        "    parser.add_argument(\"--headless\", action=\"store_true\", help=\"Run in headless mode (Don't plot samples on start)\")\n",
        "\n",
        "    parser.add_argument(\"--max_epochs\", type=int, default=100, help=\"Maximum number of epochs to train\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=6, help=\"Batch size for training and validation\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=5e-6, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=1e-4, help=\"Weight decay\")\n",
        "    parser.add_argument(\"--momentum_betas\", nargs=2, type=float, default=[0.9, 0.95],\n",
        "                        help=\"Momentum beta 1 and 2 for Adam optimizer\")\n",
        "    parser.add_argument(\"--gradient_clip_val\", type=float, default=1.0, help=\"Gradient clipping value\")\n",
        "    parser.add_argument(\"--temperature\", type=float, default=0.07, help=\"Temperature parameter for InfoNCE loss\")\n",
        "    parser.add_argument(\"--num_workers\", type=int, default=0, help=\"Number of workers for data loading\")\n",
        "    parser.add_argument(\"--self_contrast\", action=\"store_true\", help=\"Use self-contrast on the image modality\")\n",
        "\n",
        "    parser.add_argument(\"--lora\", action=\"store_true\", help=\"Use LoRA\")\n",
        "    parser.add_argument(\"--lora_rank\", type=int, default=4, help=\"Rank of LoRA layers\")\n",
        "    parser.add_argument(\"--lora_checkpoint_dir\", type=str, default=\"./.checkpoints/lora\",\n",
        "                        help=\"Directory to save LoRA checkpoint\")\n",
        "    parser.add_argument(\"--lora_modality_names\", nargs=\"+\", type=str, default=[\"vision\", \"text\", 'audio'],\n",
        "                        choices=[\"vision\", \"text\", \"audio\", \"thermal\", \"depth\", \"imu\"],\n",
        "                        help=\"Modality names to apply LoRA\")\n",
        "    parser.add_argument(\"--lora_layer_idxs\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA\")\n",
        "    parser.add_argument(\"--lora_layer_idxs_vision\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA for vision modality. Overrides lora_layer_idxs if specified\")\n",
        "    parser.add_argument(\"--lora_layer_idxs_text\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA for text modality. Overrides lora_layer_idxs if specified\")\n",
        "    parser.add_argument(\"--lora_layer_idxs_audio\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA for audio modality. Overrides lora_layer_idxs if specified\")\n",
        "    parser.add_argument(\"--lora_layer_idxs_thermal\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA for thermal modality. Overrides lora_layer_idxs if specified\")\n",
        "    parser.add_argument(\"--lora_layer_idxs_depth\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA for depth modality. Overrides lora_layer_idxs if specified\")\n",
        "    parser.add_argument(\"--lora_layer_idxs_imu\", nargs=\"+\", type=int,\n",
        "                        help=\"Layer indices to apply LoRA for imu modality. Overrides lora_layer_idxs if specified\")\n",
        "\n",
        "    parser.add_argument(\"--linear_probing\", action=\"store_true\",\n",
        "                        help=\"Freeze model and train the last layers of the head for each modality.\")\n",
        "\n",
        "    parser.set_defaults(linear_probing=True, lora=False)\n",
        "    #parser.set_defaults(linear_probing=False, lora=True)\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "id": "NbmpQwMkZZDI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # little hack\n",
        "    import gc\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    # Create loggers\n",
        "    loggers = []\n",
        "    for logger in args.loggers if args.loggers is not None else []:\n",
        "        if logger == \"wandb\":\n",
        "            wandb.init(project=\"imagebind\", config=args)\n",
        "            wandb_logger = pl_loggers.WandbLogger(\n",
        "                save_dir=args.loggers_dir,\n",
        "                name=\"imagebind\")\n",
        "            loggers.append(wandb_logger)\n",
        "        elif logger == \"tensorboard\":\n",
        "            tensorboard_logger = pl_loggers.TensorBoardLogger(\n",
        "                save_dir=args.loggers_dir,\n",
        "                name=\"imagebind\")\n",
        "            loggers.append(tensorboard_logger)\n",
        "        elif logger == \"comet\":\n",
        "            comet_logger = pl_loggers.CometLogger(\n",
        "                save_dir=args.loggers_dir,\n",
        "                api_key=os.environ[\"COMET_API_KEY\"],\n",
        "                workspace=os.environ[\"COMET_WORKSPACE\"],\n",
        "                project_name=os.environ[\"COMET_PROJECT_NAME\"],\n",
        "                experiment_name=os.environ.get(\"COMET_EXPERIMENT_NAME\", None),\n",
        "            )\n",
        "            loggers.append(comet_logger)\n",
        "        elif logger == \"mlflow\":\n",
        "            mlflow_logger = pl_loggers.MLFlowLogger(\n",
        "                save_dir=args.loggers_dir,\n",
        "                experiment_name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"],\n",
        "                tracking_uri=os.environ[\"MLFLOW_TRACKING_URI\"],\n",
        "                run_name=\"imagebind\"\n",
        "            )\n",
        "            loggers.append(mlflow_logger)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown logger: {logger}\")\n",
        "\n",
        "    # Set experiment properties\n",
        "    seed_everything(args.seed, workers=True)\n",
        "    torch.backends.cudnn.determinstic = True\n",
        "    device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    device = torch.device(device_name)\n",
        "\n",
        "    train_datasets = []\n",
        "    test_datasets = []\n",
        "\n",
        "    # lot's of data (esp. in train and test) don't have audio in .mp4 videos,\n",
        "    # so audio modality isn't used in MELD for now\n",
        "    if \"MELD\" in args.datasets:\n",
        "        from datasets.meld import MeldDataset\n",
        "\n",
        "        train_datasets.append(MeldDataset('../MELD.Raw/train/train_sent_emo.csv', split='train', for_testing=False, get_audio=False,\n",
        "                                          shuffle=True, device=device))\n",
        "        test_datasets.append(MeldDataset('../MELD.Raw/dev/dev_sent_emo.csv', split='dev', for_testing=False, get_audio=False, device=device))\n",
        "\n",
        "    if len(args.datasets) == 1:\n",
        "        train_dataset = train_datasets[0]\n",
        "        test_dataset = test_datasets[0]\n",
        "    else:\n",
        "        train_dataset = ConcatDataset(train_datasets)\n",
        "        test_dataset = ConcatDataset(test_datasets)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        pin_memory=False,\n",
        "        num_workers=args.num_workers,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        pin_memory=False,\n",
        "        num_workers=args.num_workers,\n",
        "    )\n",
        "\n",
        "    # Parse indices of layers to apply LoRA\n",
        "    lora_layer_idxs = {}\n",
        "    lora_modality_names = []\n",
        "    modalities = [\"vision\", \"text\", \"audio\", \"thermal\", \"depth\", \"imu\"]\n",
        "    for modality_name in args.lora_modality_names:\n",
        "        if modality_name in modalities:\n",
        "            modality_type = getattr(ModalityType, modality_name.upper())\n",
        "            lora_layer_idxs[modality_type] = getattr(args, f'lora_layer_idxs_{modality_name}', None)\n",
        "            if not lora_layer_idxs[modality_type]:\n",
        "                lora_layer_idxs[modality_type] = None\n",
        "            lora_modality_names.append(modality_type)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown modality name: {modality_name}\")\n",
        "\n",
        "\n",
        "    # Train dataset\n",
        "    model = ImageBindTrain(max_epochs=args.max_epochs, batch_size=args.batch_size, lr=args.lr,\n",
        "                           weight_decay=args.weight_decay, momentum_betas=args.momentum_betas,\n",
        "                           temperature=args.temperature,\n",
        "                           num_workers=args.num_workers, self_contrast=args.self_contrast,\n",
        "                           lora=args.lora, lora_rank=args.lora_rank, lora_checkpoint_dir=args.lora_checkpoint_dir,\n",
        "                           lora_layer_idxs=lora_layer_idxs if lora_layer_idxs else None,\n",
        "                           lora_modality_names=lora_modality_names if lora_modality_names else None,\n",
        "                           linear_probing=args.linear_probing)\n",
        "\n",
        "    if args.full_model_checkpointing:\n",
        "        checkpointing = {\"enable_checkpointing\": args.full_model_checkpointing,\n",
        "                         \"callbacks\": [ModelCheckpoint(monitor=\"val_loss\", dirpath=args.full_model_checkpoint_dir,\n",
        "                                                        filename=\"imagebind-{epoch:02d}-{val_loss:.2f}\",\n",
        "                                                        save_last=True, mode=\"min\")]}\n",
        "    else:\n",
        "        checkpointing = {\"enable_checkpointing\": args.full_model_checkpointing,}\n",
        "\n",
        "    trainer = Trainer(accelerator=\"gpu\" if \"cuda\" in device_name else \"cpu\",\n",
        "                      devices=1 if \":\" not in device_name else [int(device_name.split(\":\")[1])], deterministic=True,\n",
        "                      max_epochs=args.max_epochs, gradient_clip_val=args.gradient_clip_val,\n",
        "                      logger=loggers if loggers else None, **checkpointing)\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "vCmYJf7jWoyv",
        "outputId": "f8259996-c921-4f27-b68b-2f0f3bdbc4d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'L' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1b56ac0491fd>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mImageBindTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     def __init__(self, lr=5e-4, weight_decay=1e-4, max_epochs=500, batch_size=32, num_workers=4, seed=42, \n\u001b[1;32m     12\u001b[0m                  \u001b[0mself_contrast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmomentum_betas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'L' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efGtDW34WjaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5TaSru6mo0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}